{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled51.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xD442CRr4pNH",
        "colab_type": "code",
        "outputId": "f48d5d53-4de8-43dc-d436-7a6c3eb11eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dataset=\"fmnist\"\n",
        "flag1=1 #train\n",
        "flag2=1 #test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVbTbV9hBsfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKuV0WSSATVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F-mkIRd36ooG",
        "colab_type": "code",
        "outputId": "6cfad65b-51fd-47a8-8d77-db61587021b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if dataset==\"CIFAR-10\":\n",
        "   fp='/content/gdrive/My Drive/Colab Notebooks/CIFAR-10'\n",
        "else:\n",
        "  fp='/content/gdrive/My Drive/Colab Notebooks/Fashion-MNIST'\n",
        "  \n",
        "print(fp)\n",
        "def swish(x):\n",
        "  return x*tf.nn.sigmoid(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Fashion-MNIST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qY2jd6el6hCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "    path=fp\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS356VXFHlJ4",
        "colab_type": "code",
        "outputId": "9fe0caf6-69c4-48f0-ce95-897367d72742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tsne\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tsne\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/57/87d99c7c3da6e25dc7a34b7d305cf5a4f5850b78edd2a5e12de2254a1155/tsne-0.1.8.tar.gz\n",
            "Requirement already satisfied: Cython>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tsne) (0.29.6)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from tsne) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from tsne) (1.1.0)\n",
            "Building wheels for collected packages: tsne\n",
            "  Building wheel for tsne (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/b4/26/9a673d2333de6af3865b5a2b462b3b0cfe4556ae69979434dd\n",
            "Successfully built tsne\n",
            "Installing collected packages: tsne\n",
            "Successfully installed tsne-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GBk1TKHBe4sP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import zipfile\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from time import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tsne import bh_sne\n",
        "\n",
        "def get_data_set(fp,name=\"train\"):\n",
        "    x = None\n",
        "    y = None\n",
        "\n",
        "    if name is \"train\":\n",
        "        for i in range(5):\n",
        "            f = open(fp+'/data_batch_' + str(i + 1), 'rb')\n",
        "            datadict = pickle.load(f, encoding='latin1')\n",
        "            f.close()\n",
        "\n",
        "            _X = datadict[\"data\"]\n",
        "            _Y = datadict['labels']\n",
        "\n",
        "            _X = np.array(_X, dtype=float) / 255.0\n",
        "            _X = _X.reshape([-1, 3, 32, 32])\n",
        "            _X = _X.transpose([0, 2, 3, 1])\n",
        "            _X = _X.reshape(-1, 32*32*3)\n",
        "\n",
        "            if x is None:\n",
        "                x = _X\n",
        "                y = _Y\n",
        "            else:\n",
        "                x = np.concatenate((x, _X), axis=0)\n",
        "                y = np.concatenate((y, _Y), axis=0)\n",
        "\n",
        "    elif name is \"test\":\n",
        "        f = open(fp+'/test_batch', 'rb')\n",
        "        datadict = pickle.load(f, encoding='latin1')\n",
        "        f.close()\n",
        "\n",
        "        x = datadict[\"data\"]\n",
        "        y = np.array(datadict['labels'])\n",
        "\n",
        "        x = np.array(x, dtype=float) / 255.0\n",
        "        x = x.reshape([-1, 3, 32, 32])\n",
        "        x = x.transpose([0, 2, 3, 1])\n",
        "        x = x.reshape(-1, 32*32*3)\n",
        "    #print(y)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def dense_to_one_hot(labels_dense, num_classes=10):\n",
        "    #print(labels_dense.shape[0])\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = np.arange(num_labels) * num_classes\n",
        "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "\n",
        "    return labels_one_hot\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Knp65m-SHj48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convl(input_,output_dim,kernal_size,activation_):\n",
        "    conv = tf.layers.conv2d(\n",
        "            inputs=input_,\n",
        "            filters=output_dim,\n",
        "            kernel_size=[kernal_size, kernal_size],kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
        "            padding='SAME',\n",
        "            activation=activation_\n",
        "        )\n",
        "    pool= tf.layers.max_pooling2d(conv, pool_size=[2, 2], strides=2, padding='SAME')\n",
        "    norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
        "                    name='norm1')\n",
        "    return norm1\n",
        "  \n",
        "def model1(a,b,filter_list,activationfn):#a is image size,b is image channel\n",
        "    _IMAGE_SIZE = a\n",
        "    _IMAGE_CHANNELS = b\n",
        "    _NUM_CLASSES = 10\n",
        "    with tf.name_scope('main_params'):\n",
        "        x = tf.placeholder(tf.float32, shape=[None, _IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_CHANNELS], name='Input')\n",
        "        y = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='Output')\n",
        "        x_image = tf.reshape(x, [-1, _IMAGE_SIZE, _IMAGE_SIZE, _IMAGE_CHANNELS], name='images')\n",
        "\n",
        "        global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "    input_=x_image\n",
        "    \n",
        "    for i in range(len(filter_list)):\n",
        "      a=\"conv\"+str(i+1)\n",
        "      with tf.variable_scope(a) as scope:\n",
        "       \n",
        "          x_filtersize=filter_list[i];\n",
        "          conv=convl(input_,64*(i+1),x_filtersize,activationfn)\n",
        "          drop = tf.layers.dropout(conv, rate=0.25, name=scope.name)\n",
        "          input_=drop\n",
        "\n",
        "    keep_prob=.75\n",
        "    flat = tf.contrib.layers.flatten(drop)  \n",
        "\n",
        "    # 10\n",
        "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=activationfn)\n",
        "    full1 = tf.nn.dropout(full1, keep_prob)\n",
        "    full1 = tf.layers.batch_normalization(full1)\n",
        "\n",
        "    # 11\n",
        "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=activationfn)\n",
        "    full2 = tf.nn.dropout(full2, keep_prob)\n",
        "    full2 = tf.layers.batch_normalization(full2)\n",
        "\n",
        "    # 12\n",
        "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=activationfn)\n",
        "    full3 = tf.nn.dropout(full3, keep_prob)\n",
        "    full3 = tf.layers.batch_normalization(full3)    \n",
        "\n",
        "    # 13\n",
        "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=activationfn)\n",
        "    full4 = tf.nn.dropout(full4, keep_prob)\n",
        "    full4 = tf.layers.batch_normalization(full4)        \n",
        "\n",
        "    #fc = tf.layers.dense(inputs=flat, units=1500, activation=tf.nn.relu)\n",
        "    #drop = tf.layers.dropout(fc, rate=0.5)\n",
        "    softmax = tf.layers.dense(inputs=full4, units=_NUM_CLASSES, name=scope.name)\n",
        "\n",
        "    y_pred_cls = tf.argmax(softmax, axis=1)\n",
        "\n",
        "    return x, y, softmax, y_pred_cls, global_step, learning_rate,full2\n",
        "          \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCR_4eTRe4sU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def lr(epoch):\n",
        "    learning_rate = 1e-3\n",
        "    if epoch > 80:\n",
        "        learning_rate *= 0.5e-3\n",
        "    elif epoch > 60:\n",
        "        learning_rate *= 1e-4\n",
        "    elif epoch > 40:\n",
        "        learning_rate *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        learning_rate *= 1e-1\n",
        "    return learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KdN51oHmzxqK",
        "colab_type": "code",
        "outputId": "f05e5cb0-1460-4b68-a97c-d2e7d34c5fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "cell_type": "code",
      "source": [
        "#dataset=\"f\"\n",
        "#test_x, test_y = get_data_set(\"test\")\n",
        "\n",
        "tf.set_random_seed(21)\n",
        "tf.reset_default_graph()\n",
        "#just to reset the graph with same name\n",
        "#x, y, output, y_pred_cls, global_step, learning_rate,full1 = model()\n",
        "sess = tf.Session()\n",
        "if dataset==\"CIFAR-10\":\n",
        "     x, y, output, y_pred_cls, global_step, learning_rate,full2 = model1(32,3,[8,4,4],swish)\n",
        "else:\n",
        "     x, y, output, y_pred_cls, global_step, learning_rate,full2 = model1(28,1,[8,4,4],swish)\n",
        "global_accuracy = 0\n",
        "epoch_start = 0\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "_BATCH_SIZE = 128\n",
        "_EPOCH = 60\n",
        "if dataset==\"CIFAR-10\":\n",
        "   _SAVE_PATH =\"/content/gdrive/My Drive/Colab Notebooks/model_cifar/CIFAR\"\n",
        "   checkpoint_path = \"/content/gdrive/My Drive/Colab Notebooks/model_cifar/CIFAR\"\n",
        "else:\n",
        "    _SAVE_PATH =\"/content/gdrive/My Drive/Colab Notebooks/model_fmnist/FMINST\"\n",
        "    checkpoint_path = \"/content/gdrive/My Drive/Colab Notebooks/model_fmnist\"\n",
        "\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                   beta1=0.9,\n",
        "                                   beta2=0.999,\n",
        "                                   epsilon=1e-08).minimize(loss, global_step=global_step)\n",
        "\n",
        "\n",
        "# PREDICTION AND ACCURACY CALCULATION\n",
        "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:9: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:33: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:42: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-76c05f52e90b>:61: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-oONzkIX3OjN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(sess,epoch,train_x,train_y,test_x,test_y):\n",
        "    global epoch_start\n",
        "    \n",
        "    epoch_start = time()\n",
        "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
        "    i_global = 0\n",
        "\n",
        "    for s in range(batch_size):\n",
        "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "\n",
        "        start_time = time()\n",
        "        i_global, _, batch_loss, batch_acc = sess.run(\n",
        "            [global_step, optimizer, loss, accuracy],\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
        "        duration = time() - start_time\n",
        "\n",
        "        if s % 10 == 0:\n",
        "            percentage = int(round((s/batch_size)*100))\n",
        "\n",
        "            bar_len = 29\n",
        "            filled_len = int((bar_len*int(percentage))/100)\n",
        "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
        "\n",
        "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\n",
        "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
        "\n",
        "    return test_and_save(sess,i_global, epoch,test_x,test_y)\n",
        "\n",
        "\n",
        "def test_and_save(sess,_global_step, epoch,test_x,test_y):\n",
        "    global global_accuracy\n",
        "    global epoch_start\n",
        "    global F_micro\n",
        "    global F_macro\n",
        "\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x):\n",
        "        j = min(i + _BATCH_SIZE, len(test_x))\n",
        "        batch_xs = test_x[i:j, :]\n",
        "        batch_ys = test_y[i:j, :]\n",
        "        predicted_class[i:j] = sess.run(\n",
        "            y_pred_cls,\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)}\n",
        "        )\n",
        "        i = j\n",
        "    actual_=np.argmax(test_y, axis=1)\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "    \n",
        "    acc = correct.mean()*100\n",
        "    micro=f1_score(actual_,predicted_class, average='micro')\n",
        "    macro=f1_score(actual_,predicted_class, average='macro')\n",
        "    correct_numbers = correct.sum()\n",
        "\n",
        "    hours, rem = divmod(time() - epoch_start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    mes = \"\\nEpoch {} - accuracy: {:.2f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "    print(mes.format((epoch+1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\n",
        "    train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
        "    if global_accuracy != 0 and global_accuracy < acc:\n",
        "\n",
        "        summary = tf.Summary(value=[\n",
        "            tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc),\n",
        "        ])\n",
        "        train_writer.add_summary(summary)\n",
        "\n",
        "        saver.save(sess, save_path=_SAVE_PATH)\n",
        "\n",
        "        mes = \"This epoch receive better accuracy: {:.2f} > {:.2f}. Saving session...\"\n",
        "        print(mes.format(acc, global_accuracy))\n",
        "        global_accuracy = acc\n",
        "        F_micro=micro\n",
        "        F_macro=macro\n",
        "\n",
        "    elif global_accuracy == 0:\n",
        "        global_accuracy = acc\n",
        "        F_micro=micro\n",
        "        F_macro=macro\n",
        "\n",
        "    print(\"###########################################################################################################\")\n",
        "    return acc,micro,macro,global_accuracy,F_micro,F_macro\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OArqvan3KxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(sess,test_x,test_y):\n",
        "        #test_x, test_y = get_data_set(fp,\"test\")\n",
        "        #x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "\n",
        "\n",
        "        _BATCH_SIZE = test_x.shape[0]\n",
        "        _CLASS_SIZE = 10\n",
        "        #_SAVE_PATH ='/content/gdrive/My Drive/Colab Notebooks/CIFAR'\n",
        "\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "\n",
        "        #try:\n",
        "        print(\"\\nTrying to restore last checkpoint ...\")\n",
        "        #last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "        #print(_SAVE_PATH, last_chk_path)\n",
        "        saver.restore(sess, save_path=_SAVE_PATH)\n",
        "        #saver.restore(sess, save_path='%s-15614' % _SAVE_PATH)        \n",
        "        #print(\"Restored checkpoint from:\", '%s-15614' % _SAVE_PATH)\n",
        "        #except ValueError:\n",
        "            #print(\"\\nFailed to restore checkpoint. Initializing variables instead.\")\n",
        "            #sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "        def main():\n",
        "            i = 0\n",
        "\n",
        "\n",
        "            predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "           \n",
        "            a,b = sess.run([y_pred_cls,full2], feed_dict={x: test_x, y: test_y})\n",
        "            predicted_class=a\n",
        "            actual_=np.argmax(test_y, axis=1)\n",
        "            print(b.shape)\n",
        "            n_sne = 5000\n",
        "            b = np.asarray(b).astype('float64')\n",
        "         \n",
        "            f_tsne=bh_sne(b)\n",
        "            plt.scatter(f_tsne[:,0], f_tsne[:,1], c=actual_, cmap=plt.cm.get_cmap(\"jet\", 10))\n",
        "            plt.colorbar(ticks=range(10))\n",
        "            plt.clim(-0.5, 9.5)\n",
        "            plt.show()\n",
        "            plt.savefig('t_sne_1.png')\n",
        "\n",
        "            from collections import defaultdict\n",
        "            kmeans = KMeans(n_clusters=10, random_state=0).fit_transform(b)\n",
        "            #print(kmeans[1][1])\n",
        "            #ids_=np.arange(10)\n",
        "            correct = 0\n",
        "            label=defaultdict(list)\n",
        "            for i in range(kmeans.shape[0]):\n",
        "              label[np.argmax(kmeans[i])].append(actual_[i])\n",
        "            z_=np.zeros(10,dtype=int)\n",
        "            for i in range(10):\n",
        "              z__=np.zeros(10,dtype=int)\n",
        "              for j in range(len(label[i])):\n",
        "                z__[label[i][j]]+=1\n",
        "              z_[i]+=max(z__)\n",
        "            print(np.sum(z_)/float(len(b)))\n",
        "            \n",
        "            \n",
        "              \n",
        "                \n",
        "            \n",
        "                #predict_me = predict_me.reshape(-1, len(predict_me))\n",
        "            \"\"\"\n",
        "            for i in range(len(b)):\n",
        "                predict_me = f_tsne[i]\n",
        "                predict_me = predict_me.reshape(-1, len(predict_me))\n",
        "                prediction = kmeans.predict(predict_me)\n",
        "                x__=prediction[0]\n",
        "                permu.append(x__)\n",
        "              \n",
        "            correct,correctlabel=maxacc(permu,actual_)\n",
        "            print(correct)\n",
        "            \"\"\"\n",
        "\n",
        "            print(\"f_tsne shape\",f_tsne.shape)\n",
        "\n",
        "            \n",
        "            correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "            acc = correct.mean() * 100\n",
        "            micro=f1_score(actual_,predicted_class, average='micro')\n",
        "            macro=f1_score(actual_,predicted_class, average='macro')\n",
        "            \n",
        "            \n",
        "            correct_numbers = correct.sum()\n",
        "            print()\n",
        "            print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc, correct_numbers, len(test_x)))\n",
        "            print(\"MICRO ON TEST_SET\",micro)\n",
        "            print(\"MACRO ON TEST_SET\",macro)\n",
        "\n",
        "\n",
        "\n",
        "        if __name__ == \"__main__\":\n",
        "            main()\n",
        "\n",
        "\n",
        "        sess.close()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbz_e10E-_Fw",
        "colab_type": "code",
        "outputId": "499c13d9-f7d0-4a42-bac5-d2a461586d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(_SAVE_PATH)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/model_fmnist/FMINST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VoB9DaOXe4sX",
        "colab_type": "code",
        "outputId": "738eddf6-36c1-4b80-cbc6-94ab028eca68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1275
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    train_start = time()\n",
        "    microl=[]\n",
        "    accl=[]\n",
        "    macrol=[]\n",
        "    epochl=[]\n",
        "    if flag1==1:\n",
        "      if dataset==\"CIFAR-10\":\n",
        "          train_x, train_y = get_data_set(fp,\"train\")\n",
        "      else:\n",
        "          train_x, train_y = load_mnist(fp,\"train\")\n",
        "      train_x, test_x,train_y,test_y = train_test_split(train_x, train_y, test_size=0.1, random_state=42)\n",
        "      test_y=dense_to_one_hot(test_y)\n",
        "      train_y=dense_to_one_hot(train_y)\n",
        "      #test(test_x,test_y)\n",
        "\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      for i in range(_EPOCH):\n",
        "          print(\"\\nEpoch: {}/{}\\n\".format((i+1), _EPOCH))\n",
        "          acc,micro,macro,a,b,c=train(sess,i,train_x,train_y,test_x,test_y)\n",
        "          print(acc,micro,macro)\n",
        "          microl.append(micro)\n",
        "          accl.append(acc)\n",
        "          macrol.append(macro)\n",
        "          epochl.append(i)\n",
        "      plt.plot(epochl, microl, label='F1_micro')\n",
        "      plt.show()\n",
        "      plt.plot(epochl, macrol, label='F1_macro')\n",
        "      plt.show()\n",
        "      plt.plot(epochl, accl, label='accuracy')\n",
        "      plt.show()\n",
        "      hours, rem = divmod(time() - train_start, 3600)\n",
        "      minutes, seconds = divmod(rem, 60)\n",
        "      mes = \"Best accuracy pre session: {:.2f}, time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "      print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\n",
        "    if flag2==1:\n",
        "      if dataset==\"CIFAR-10\":\n",
        "          train_x, train_y = get_data_set(fp,\"train\")\n",
        "      else:\n",
        "          train_x, train_y = load_mnist(fp,\"train\")\n",
        "      test(sess,train_x,train_y)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/60\n",
            "\n",
            "Global step:     1 - [>-----------------------------]   0% - acc: 0.1016 - loss: 3.5268 - 41.1 sample/sec\n",
            "Global step:    11 - [>-----------------------------]   2% - acc: 0.2969 - loss: 1.7570 - 2270.3 sample/sec\n",
            "Global step:    21 - [=>----------------------------]   5% - acc: 0.3750 - loss: 1.3949 - 2282.6 sample/sec\n",
            "Global step:    31 - [==>---------------------------]   7% - acc: 0.5312 - loss: 1.2109 - 2224.0 sample/sec\n",
            "Global step:    41 - [==>---------------------------]   9% - acc: 0.5938 - loss: 1.0139 - 2178.7 sample/sec\n",
            "Global step:    51 - [===>--------------------------]  12% - acc: 0.6641 - loss: 0.8992 - 2268.1 sample/sec\n",
            "Global step:    61 - [====>-------------------------]  14% - acc: 0.6719 - loss: 0.8094 - 2286.6 sample/sec\n",
            "Global step:    71 - [====>-------------------------]  17% - acc: 0.7812 - loss: 0.6369 - 2248.5 sample/sec\n",
            "Global step:    81 - [=====>------------------------]  19% - acc: 0.6875 - loss: 0.8026 - 2300.9 sample/sec\n",
            "Global step:    91 - [======>-----------------------]  21% - acc: 0.6484 - loss: 0.7882 - 2210.0 sample/sec\n",
            "Global step:   101 - [======>-----------------------]  24% - acc: 0.7031 - loss: 0.6832 - 2266.3 sample/sec\n",
            "Global step:   111 - [=======>----------------------]  26% - acc: 0.7500 - loss: 0.6721 - 2248.7 sample/sec\n",
            "Global step:   121 - [========>---------------------]  28% - acc: 0.7656 - loss: 0.5702 - 2253.7 sample/sec\n",
            "Global step:   131 - [========>---------------------]  31% - acc: 0.7891 - loss: 0.7030 - 2297.1 sample/sec\n",
            "Global step:   141 - [=========>--------------------]  33% - acc: 0.7500 - loss: 0.5841 - 2242.5 sample/sec\n",
            "Global step:   151 - [==========>-------------------]  36% - acc: 0.7422 - loss: 0.6760 - 2233.3 sample/sec\n",
            "Global step:   161 - [===========>------------------]  38% - acc: 0.7656 - loss: 0.7137 - 2290.0 sample/sec\n",
            "Global step:   171 - [===========>------------------]  40% - acc: 0.8359 - loss: 0.5693 - 2291.2 sample/sec\n",
            "Global step:   181 - [============>-----------------]  43% - acc: 0.8125 - loss: 0.5284 - 2263.6 sample/sec\n",
            "Global step:   191 - [=============>----------------]  45% - acc: 0.8594 - loss: 0.4337 - 2309.0 sample/sec\n",
            "Global step:   201 - [=============>----------------]  47% - acc: 0.8281 - loss: 0.4230 - 2241.9 sample/sec\n",
            "Global step:   211 - [==============>---------------]  50% - acc: 0.8750 - loss: 0.3848 - 2271.7 sample/sec\n",
            "Global step:   221 - [===============>--------------]  52% - acc: 0.8516 - loss: 0.3775 - 2254.3 sample/sec\n",
            "Global step:   231 - [===============>--------------]  55% - acc: 0.8516 - loss: 0.4276 - 2233.9 sample/sec\n",
            "Global step:   241 - [================>-------------]  57% - acc: 0.8594 - loss: 0.4382 - 2275.8 sample/sec\n",
            "Global step:   251 - [=================>------------]  59% - acc: 0.7969 - loss: 0.5198 - 2255.0 sample/sec\n",
            "Global step:   261 - [=================>------------]  62% - acc: 0.8828 - loss: 0.3799 - 2251.1 sample/sec\n",
            "Global step:   271 - [==================>-----------]  64% - acc: 0.8125 - loss: 0.4978 - 2230.4 sample/sec\n",
            "Global step:   281 - [===================>----------]  66% - acc: 0.8516 - loss: 0.3953 - 2268.6 sample/sec\n",
            "Global step:   291 - [====================>---------]  69% - acc: 0.7812 - loss: 0.6455 - 2258.0 sample/sec\n",
            "Global step:   301 - [====================>---------]  71% - acc: 0.8828 - loss: 0.3212 - 2268.7 sample/sec\n",
            "Global step:   311 - [=====================>--------]  73% - acc: 0.8750 - loss: 0.4999 - 2231.9 sample/sec\n",
            "Global step:   321 - [======================>-------]  76% - acc: 0.8125 - loss: 0.3999 - 2271.2 sample/sec\n",
            "Global step:   331 - [======================>-------]  78% - acc: 0.8984 - loss: 0.2704 - 2241.7 sample/sec\n",
            "Global step:   341 - [=======================>------]  81% - acc: 0.8359 - loss: 0.4665 - 2236.6 sample/sec\n",
            "Global step:   351 - [========================>-----]  83% - acc: 0.8359 - loss: 0.4432 - 2304.6 sample/sec\n",
            "Global step:   361 - [========================>-----]  85% - acc: 0.8438 - loss: 0.4311 - 2252.9 sample/sec\n",
            "Global step:   371 - [=========================>----]  88% - acc: 0.8281 - loss: 0.5228 - 2261.8 sample/sec\n",
            "Global step:   381 - [==========================>---]  90% - acc: 0.8750 - loss: 0.3602 - 2271.7 sample/sec\n",
            "Global step:   391 - [==========================>---]  92% - acc: 0.8594 - loss: 0.4568 - 2239.0 sample/sec\n",
            "Global step:   401 - [===========================>--]  95% - acc: 0.8906 - loss: 0.3906 - 2273.6 sample/sec\n",
            "Global step:   411 - [============================>-]  97% - acc: 0.7969 - loss: 0.5601 - 2258.7 sample/sec\n",
            "Global step:   421 - [=============================>] 100% - acc: 0.8672 - loss: 0.4249 - 2277.0 sample/sec\n",
            "\n",
            "Epoch 1 - accuracy: 85.97% (5158/6000) - time: 00:00:28.12\n",
            "###########################################################################################################\n",
            "85.96666666666667 0.8596666666666667 0.8618688439259016\n",
            "\n",
            "Epoch: 2/60\n",
            "\n",
            "Global step:   423 - [>-----------------------------]   0% - acc: 0.8281 - loss: 0.4888 - 2252.4 sample/sec\n",
            "Global step:   433 - [>-----------------------------]   2% - acc: 0.8750 - loss: 0.4017 - 2231.0 sample/sec\n",
            "Global step:   443 - [=>----------------------------]   5% - acc: 0.8281 - loss: 0.4985 - 2272.1 sample/sec\n",
            "Global step:   453 - [==>---------------------------]   7% - acc: 0.8984 - loss: 0.2824 - 2244.5 sample/sec\n",
            "Global step:   463 - [==>---------------------------]   9% - acc: 0.9141 - loss: 0.3209 - 2236.4 sample/sec\n",
            "Global step:   473 - [===>--------------------------]  12% - acc: 0.8672 - loss: 0.3675 - 2223.2 sample/sec\n",
            "Global step:   483 - [====>-------------------------]  14% - acc: 0.9219 - loss: 0.3118 - 2279.1 sample/sec\n",
            "Global step:   493 - [====>-------------------------]  17% - acc: 0.8750 - loss: 0.3477 - 2264.2 sample/sec\n",
            "Global step:   503 - [=====>------------------------]  19% - acc: 0.8750 - loss: 0.3468 - 2297.4 sample/sec\n",
            "Global step:   513 - [======>-----------------------]  21% - acc: 0.8984 - loss: 0.3417 - 2260.5 sample/sec\n",
            "Global step:   523 - [======>-----------------------]  24% - acc: 0.8984 - loss: 0.3360 - 2300.3 sample/sec\n",
            "Global step:   533 - [=======>----------------------]  26% - acc: 0.9141 - loss: 0.3380 - 2306.2 sample/sec\n",
            "Global step:   543 - [========>---------------------]  28% - acc: 0.8984 - loss: 0.2635 - 2266.8 sample/sec\n",
            "Global step:   553 - [========>---------------------]  31% - acc: 0.8594 - loss: 0.4188 - 2280.7 sample/sec\n",
            "Global step:   563 - [=========>--------------------]  33% - acc: 0.8906 - loss: 0.3943 - 2291.9 sample/sec\n",
            "Global step:   573 - [==========>-------------------]  36% - acc: 0.8828 - loss: 0.3536 - 2222.8 sample/sec\n",
            "Global step:   583 - [===========>------------------]  38% - acc: 0.8750 - loss: 0.3385 - 2277.3 sample/sec\n",
            "Global step:   593 - [===========>------------------]  40% - acc: 0.8594 - loss: 0.4727 - 2295.8 sample/sec\n",
            "Global step:   603 - [============>-----------------]  43% - acc: 0.8828 - loss: 0.3094 - 2265.8 sample/sec\n",
            "Global step:   613 - [=============>----------------]  45% - acc: 0.8984 - loss: 0.2528 - 2239.2 sample/sec\n",
            "Global step:   623 - [=============>----------------]  47% - acc: 0.8828 - loss: 0.3085 - 2281.2 sample/sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rVl2zkyuB0Se",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}